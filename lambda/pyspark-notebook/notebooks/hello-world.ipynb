{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5140d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = \"hdfs://namenode:8020/spark/checkpoint\"\n",
    "\n",
    "DAY_TYPE_WEEKDAY = 0\n",
    "DAY_TYPE_WEEKEND = 1\n",
    "DATA_ACTUAL_TIMEZONE = \"America/Los_Angeles\"\n",
    "\n",
    "BOOTSTRAP_SERVER = \"kafka:29092\"\n",
    "TOPIC = \"buses-location\"\n",
    "\n",
    "POSTGRES_URL = \"jdbc:postgresql://timescaledb:5432/lametro\"\n",
    "POSTGRES_TABLE_BUS_VELOCITY = \"bus_velocity\"\n",
    "POSTGRES_TABLE_BUS_ARRIVAL = \"bus_arrival\"\n",
    "POSTGRES_USERNAME = \"postgres\"\n",
    "POSTGRES_PASSWORD = \"8zr7E3SV\"\n",
    "\n",
    "REDIS_HOST = \"redis\"\n",
    "REDIS_PORT = \"6379\"\n",
    "REDIS_PASSWORD = \"8zr7E3SV\"\n",
    "\n",
    "STATIC_DATA_DIR = \"hdfs://namenode:8020/ola/static_data/\"\n",
    "HISTORICAL_DATA_DIR = \"hdfs://namenode:8020/ola/historical_data/\"\n",
    "AGGREGATED_DATA_DIR = \"hdfs://namenode:8020/ola/aggregated_data/\"\n",
    "TEMP_DIR = \"hdfs://namenode:8020/temp\"\n",
    "\n",
    "LOCAL_STATIC_DATA_DIR = \"/home/data/static_data/\"\n",
    "LOCAL_HISTORICAL_DATA_DIR = \"/home/data/historical_data/\"\n",
    "LOCAL_AGGREGATED_DATA_DIR = \"/home/data/aggregated_data/\"\n",
    "LOCAL_TEMP_DIR = \"/home/data/temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955dc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import  SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Spark session & context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"spark://0.0.0.0:7077\").setAppName(\"hello-world\")\n",
    "conf.set(\"spark.cores.max\", \"1\")\n",
    "conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/third-party-jars/*\")\n",
    "conf.set(\"spark.executor.extraClassPath\", \"/usr/local/spark/third-party-jars/*\")\n",
    "conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "conf.set(\"spark.ui.port\", \"4040\")\n",
    "conf.set(\"spark.redis.host\", REDIS_HOST)\n",
    "conf.set(\"spark.redis.port\", REDIS_PORT)\n",
    "conf.set(\"spark.redis.auth\", REDIS_PASSWORD)\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5ca8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "299f17ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       ".container { \n",
       "    width:95% !important; \n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    ".container { \n",
    "    width:95% !important; \n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d8b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb674128",
   "metadata": {},
   "source": [
    "# Read Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b8641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "| key|               value|         topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "|4132|{\"route_id\":\"854\"...|buses-location|        5|     0|2021-07-03 11:19:...|            0|\n",
      "+----+--------------------+--------------+---------+------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subscribe to 1 topic defaults to the earliest and latest offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\", \"buses-location\") \\\n",
    "  .load()\n",
    "\n",
    "df = df\\\n",
    "    .withColumn('key', df.key.cast(StringType()))\\\n",
    "    .withColumn('value', df.value.cast(StringType()))\\\n",
    "\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7ba137",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"route_id\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"run_id\", StringType(), True),\n",
    "    StructField(\"predictable\", BooleanType(), True),\n",
    "    StructField(\"seconds_since_report\", LongType(), True),\n",
    "    StructField(\"heading\", DoubleType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2c5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----+--------+-----------+--------------------+-------+---------+-----------+\n",
      "|timestamp |route_id|id  |run_id  |predictable|seconds_since_report|heading|latitude |longitude  |\n",
      "+----------+--------+----+--------+-----------+--------------------+-------+---------+-----------+\n",
      "|1625311195|854     |4132|854_34_1|true       |23                  |281.0  |34.054241|-118.233482|\n",
      "+----------+--------+----+--------+-----------+--------------------+-------+---------+-----------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- route_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- predictable: boolean (nullable = true)\n",
      " |-- seconds_since_report: long (nullable = true)\n",
      " |-- heading: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = df\\\n",
    "    .select(col(\"timestamp\").cast(\"long\"), from_json(\"value\", schema).alias(\"bus_location\"))\\\n",
    "    .select(\"timestamp\", \"bus_location.*\")\n",
    "\n",
    "df_1.show(1, False)\n",
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d6c7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1\\\n",
    "#     .groupBy(\"\")\n",
    "#     .write\\\n",
    "#     .format(\"org.apache.spark.sql.redis\")\\\n",
    "#     .option(\"key.column\", \"route_id\")\\\n",
    "#     .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06b6c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_maps = udf(\n",
    "    lambda maps: {key:f[key] for f in maps for key in f},\n",
    "    MapType(\n",
    "        StringType(),\n",
    "        StringType()\n",
    "    )\n",
    ")\n",
    "\n",
    "df_2 = df_1\\\n",
    "    .limit(100)\\\n",
    "    .select(\"route_id\", create_map(concat_ws(\"_\", \"id\", \"timestamp\").alias(\"id\"), (col(\"timestamp\") - col(\"seconds_since_report\")).cast(\"string\").alias(\"report_time\")).alias(\"bus\"))\\\n",
    "    .groupBy(\"route_id\")\\\n",
    "    .agg(combine_maps(collect_list(\"bus\")).alias(\"buses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b332191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jvm.com.github.lhvubtqn.spark.Utils.toRedisHashes(sc._jsc, df_2._jdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
