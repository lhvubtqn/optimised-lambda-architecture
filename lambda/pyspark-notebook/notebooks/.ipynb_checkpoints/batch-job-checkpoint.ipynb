{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2b7690",
   "metadata": {},
   "source": [
    "# Init PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd68b415",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=batch-job, master=spark://0.0.0.0:7077) created by __init__ at <ipython-input-1-a0e7ddcd1b75>:17 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a0e7ddcd1b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.ui.port\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"4041\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=batch-job, master=spark://0.0.0.0:7077) created by __init__ at <ipython-input-1-a0e7ddcd1b75>:17 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import  SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Spark session & context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"spark://0.0.0.0:7077\").setAppName(\"batch-job\")\n",
    "conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/third-party-jars/*\")\n",
    "conf.set(\"spark.executor.extraClassPath\", \"/usr/local/spark/third-party-jars/*\")\n",
    "conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "conf.set(\"spark.ui.port\", \"4041\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f5fa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       ".container { \n",
       "    width:95% !important; \n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    ".container { \n",
    "    width:95% !important; \n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964e372",
   "metadata": {},
   "source": [
    "# Define static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bf0be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_TYPE_WEEKDAY = 0\n",
    "DAY_TYPE_WEEKEND = 1\n",
    "DATA_ACTUAL_TIMEZONE = \"America/Los_Angeles\"\n",
    "\n",
    "MAX_SECONDS_SINCE_REPORT = 300\n",
    "MIN_VELOCITY = 1 # m/s\n",
    "MAX_VELOCITY = 18 # m/s\n",
    "\n",
    "STATIC_DATA_DIR = \"hdfs://namenode:8020/ola/static_data/\"\n",
    "HISTORICAL_DATA_DIR = \"hdfs://namenode:8020/ola/historical_data/\"\n",
    "AGGREGATED_DATA_DIR = \"hdfs://namenode:8020/ola/aggregated_data/\"\n",
    "\n",
    "LOCAL_STATIC_DATA_DIR = \"/home/data/static_data/\"\n",
    "LOCAL_HISTORICAL_DATA_DIR = \"/home/data/historical_data_24h/\"\n",
    "LOCAL_AGGREGATED_DATA_DIR = \"/home/data/aggregated_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d862f1",
   "metadata": {},
   "source": [
    "# Define utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06de0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd6794",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7d4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "\n",
    "# check if segment is not an empty segment (two vertex is the same)\n",
    "def fis_not_the_same(from_latlon, to_latlon):\n",
    "    return (from_latlon[0] != to_latlon[0]) | (from_latlon[1] != to_latlon[1])\n",
    "\n",
    "def ffile_path_to_ts(file_path):\n",
    "    return int(file_path[-18:-4])\n",
    "\n",
    "def fget_day_type(epoch_seconds):\n",
    "    dt = datetime.fromtimestamp(epoch_seconds)\n",
    "    wd = dt.astimezone(timezone(DATA_ACTUAL_TIMEZONE)).weekday()\n",
    "    return DAY_TYPE_WEEKDAY if wd < 5 else DAY_TYPE_WEEKEND\n",
    "\n",
    "def fhaversine_meter(lat1, lon1, lat2, lon2):\n",
    "    # distance between latitudes and longitudes\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    "\n",
    "    # convert to radians\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    " \n",
    "    # apply formulae\n",
    "    a = (math.pow(math.sin(dLat / 2), 2) +\n",
    "         math.pow(math.sin(dLon / 2), 2) *\n",
    "             math.cos(lat1) * math.cos(lat2));\n",
    "    rad = 6371\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c * 1000\n",
    "\n",
    "def ffind_distance(lat1, lon1, flat1, flon1, slat1, slon1, cl1, lat2, lon2, flat2, flon2, slat2, slon2, cl2):\n",
    "    if cl1 == cl2:\n",
    "        return fhaversine_meter(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "    if cl1 > cl2:\n",
    "        return ffind_distance(lat2, lon2, flat2, flon2, slat2, slon2, cl2, lat1, lon1, flat1, flon1, slat1, slon1, cl1)\n",
    "\n",
    "    return fhaversine_meter(lat1, lon1, slat1, slon1) + cl2 - cl1 - fhaversine_meter(flat2, flon2, slat2, slon2) + fhaversine_meter(flat2, flon2, lat2, lon2)\n",
    "\n",
    "def ffind_velocity_sign(cl1, cl2, lat1, lon1, lat2, lon2, flat, flon, slat, slon):\n",
    "    if cl1 == cl2:\n",
    "        d1 = fhaversine_meter(lat1, lon1, flat, flon)\n",
    "        d2 = fhaversine_meter(lat2, lon2, slat, slon)\n",
    "        d = fhaversine_meter(flat, flon, slat, slon)\n",
    "        return 1 if d1 + d2 < d else -1\n",
    "    return 1 if cl2 > cl1 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8791d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffind_distance: 3913.794621512769 m\n"
     ]
    }
   ],
   "source": [
    "print(\"ffind_distance:\", ffind_distance(33.82962, -118.290314, 33.8292539188, -118.2902737058, 33.8296908268, -118.2902719251, 23232.5956910088, 33.794449, -118.290871, 33.7932864432, -118.2908173976, 33.7945806653, -118.2908159585, 19326.593863383834), \"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ed28db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = datetime.fromtimestamp(1624714356)\n",
    "dt.astimezone(timezone(DATA_ACTUAL_TIMEZONE)).weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296a056",
   "metadata": {},
   "source": [
    "## Udf functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2aaff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf\n",
    "is_not_the_same = udf(lambda a, b: ffis_not_the_same(a, b), BooleanType())\n",
    "\n",
    "file_path_to_ts = udf(lambda a: ffile_path_to_ts(a), LongType())\n",
    "\n",
    "get_day_type = udf(lambda a: fget_day_type(a), IntegerType())\n",
    "\n",
    "haversine_meter = udf(lambda a, b, c, d: fhaversine_meter(a, b, c, d), DoubleType())\n",
    "\n",
    "find_distance = udf(lambda a, b, c, d, e, f, g, h, p, q, r, s, t, u: ffind_distance(a, b, c, d, e, f, g, h, p, q, r, s, t, u), DoubleType())\n",
    "\n",
    "find_velocity_sign = udf(lambda a, b, c, d, e, f, g, h, p, q: ffind_velocity_sign(a, b, c, d, e, f, g, h, p, q), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a522d1",
   "metadata": {},
   "source": [
    "# Load static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5435de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- segment_len_meter: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_sequence: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "routes_segments = spark\\\n",
    "    .read\\\n",
    "    .json(STATIC_DATA_DIR + \"route_segments.json\")\\\n",
    "    .withColumn(\"route_id\", col(\"route_id\").cast(\"int\").alias(\"route_id\"))\n",
    "\n",
    "routes_segments.cache()\n",
    "routes_segments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e1f16",
   "metadata": {},
   "source": [
    "# Load and process historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cc2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- segment_len_meter: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_sequence: long (nullable = true)\n",
      " |-- bus_segment_distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Load data with filename as request time\n",
    "# bus_positions = spark\\\n",
    "#     .read\\\n",
    "#     .json(LOCAL_HISTORICAL_DATA_DIR + \"**/*\")\\\n",
    "#     .withColumn(\"filepath\", input_file_name())\\\n",
    "#     .select(substring(\"filepath\", -13, 10).alias(\"req_time\"), explode(\"items\").alias(\"vehicle_position\"))\\\n",
    "#     .select(col(\"req_time\").cast(\"long\"), col(\"vehicle_position.*\"))\\\n",
    "\n",
    "# Load data\n",
    "bus_positions = spark\\\n",
    "    .read\\\n",
    "    .json(HISTORICAL_DATA_DIR + \"*\")\\\n",
    "    .dropna(\"any\")\n",
    "\n",
    "# Cast fields to int\n",
    "bus_positions = bus_positions\\\n",
    "    .select(\n",
    "        (col(\"timestamp\") - col(\"seconds_since_report\")).alias(\"timestamp\"),\n",
    "        col(\"id\").cast(\"int\").alias(\"bus_id\"),\n",
    "        col(\"route_id\").cast(\"int\").alias(\"route_id\"),\n",
    "        \"run_id\", \"latitude\", \"longitude\"\n",
    "    )\\\n",
    "\n",
    "# With positions having same bus_id, run_id, latitude, longitude, order them by timestamp then drop consecutive duplicate rows\n",
    "bus_positions = bus_positions\\\n",
    "   .select(\n",
    "        \"bus_id\", \"latitude\", \"longitude\", \"route_id\", \"run_id\", \"timestamp\",\n",
    "        *[lag(c).over(Window.partitionBy(\"bus_id\", \"route_id\", \"run_id\").orderBy(\"timestamp\")).alias(\"prev_\" + c) for c in [\"latitude\", \"longitude\", \"timestamp\"]]\n",
    "    )\\\n",
    "    .where((col(\"prev_latitude\").isNull()) | (col(\"latitude\") != col(\"prev_latitude\")) | (col(\"longitude\") != col(\"prev_longitude\")))\\\n",
    "    .select(\"bus_id\", \"latitude\", \"longitude\", \"route_id\", \"run_id\", \"timestamp\")\n",
    "\n",
    "# add day_type field, 0 if is weekday, 1 if is weekend or holiday\n",
    "bus_positions = bus_positions\\\n",
    "    .withColumn(\"day_type\", get_day_type(\"timestamp\"))\n",
    "\n",
    "# get segment of bus positions\n",
    "bus_positions = bus_positions\\\n",
    "    .join(routes_segments, \"route_id\")\\\n",
    "    .withColumn(\"bus_segment_distance\", haversine_meter(\"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\") + haversine_meter(\"latitude\", \"longitude\", \"segment_second_lat\", \"segment_second_lon\") - col(\"segment_len_meter\"))\\\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"bus_id\", \"route_id\", \"latitude\", \"longitude\", \"run_id\", \"timestamp\").orderBy(asc(\"bus_segment_distance\"))))\\\n",
    "    .where(\"row_number = 1\")\\\n",
    "    .drop(\"row_number\")\\\n",
    "\n",
    "# drop rows with at least one null column\n",
    "bus_positions = bus_positions.dropna(\"any\")\n",
    "\n",
    "bus_positions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b339afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- next_timestamp: long (nullable = true)\n",
      " |-- next_latitude: double (nullable = true)\n",
      " |-- next_longitude: double (nullable = true)\n",
      " |-- next_segment_first_lat: double (nullable = true)\n",
      " |-- next_segment_first_lon: double (nullable = true)\n",
      " |-- next_segment_second_lat: double (nullable = true)\n",
      " |-- next_segment_second_lon: double (nullable = true)\n",
      " |-- next_segment_cum_len: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with bus positions having same \"bus_id\", \"route_id\", \"run_id\", \"day_type\", sort them in ascending order by \"timestamp\",\n",
    "# then create a segment with every 2 consecutive rows with same \"run_id\".\n",
    "# This ensure that we only calculate velocity for bus positions in the same trip.\n",
    "bus_segments = bus_positions\\\n",
    "    .select(\n",
    "        \"bus_id\", \"route_id\", \"day_type\", \"run_id\", \"timestamp\", \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\", \"segment_id\",\n",
    "        *[lead(column_name).over(Window.partitionBy(\"bus_id\", \"route_id\", \"day_type\", \"run_id\").orderBy(\"timestamp\")).alias(\"next_\" + column_name) for column_name in [\"timestamp\", \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\"]]\n",
    "    )\\\n",
    "    .dropna(\"any\")\\\n",
    "\n",
    "bus_segments.cache()\n",
    "bus_segments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31bf91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- next_timestamp: long (nullable = true)\n",
      " |-- next_latitude: double (nullable = true)\n",
      " |-- next_longitude: double (nullable = true)\n",
      " |-- next_segment_first_lat: double (nullable = true)\n",
      " |-- next_segment_first_lon: double (nullable = true)\n",
      " |-- next_segment_second_lat: double (nullable = true)\n",
      " |-- next_segment_second_lon: double (nullable = true)\n",
      " |-- next_segment_cum_len: double (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- delta_time: long (nullable = true)\n",
      " |-- velocity_sign: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with each segment, calculate distance and delta time\n",
    "# then find the sign of velocity of this bus on this segment,\n",
    "# 0 means the bus is running alongside with the segment sequence id\n",
    "bus_segments_1 = bus_segments\\\n",
    "    .withColumn(\n",
    "        \"distance\", \n",
    "        find_distance(\n",
    "            \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \n",
    "            \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\", \n",
    "            \"next_latitude\", \"next_longitude\", \"next_segment_first_lat\", \n",
    "            \"next_segment_first_lon\", \"next_segment_second_lat\", \n",
    "            \"next_segment_second_lon\", \"next_segment_cum_len\"\n",
    "        )\n",
    "    )\\\n",
    "    .where(col(\"distance\") != 0)\\\n",
    "    .withColumn(\"delta_time\", col(\"next_timestamp\") - col(\"timestamp\"))\\\n",
    "    .where(col(\"delta_time\") <= MAX_SECONDS_SINCE_REPORT)\\\n",
    "    .where((col(\"distance\") / col(\"delta_time\") >= MIN_VELOCITY) & (col(\"distance\") / col(\"delta_time\") <= MAX_VELOCITY))\\\n",
    "    .withColumn(\"velocity_sign\", find_velocity_sign(\"segment_cum_len\", \"next_segment_cum_len\", \"latitude\", \"longitude\", \"next_latitude\", \"next_longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\"))\\\n",
    "    \n",
    "bus_segments_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a309bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- velocity_sign: integer (nullable = true)\n",
      " |-- total_distance: double (nullable = true)\n",
      " |-- total_delta_time: long (nullable = true)\n",
      " |-- velocity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bus_velocities = bus_segments_1\\\n",
    "    .withColumn(\"direction\", substring(\"run_id\", -1, 1))\\\n",
    "    .groupBy(\"route_id\", \"direction\", \"day_type\", \"velocity_sign\")\\\n",
    "    .agg(sum(\"distance\").alias(\"total_distance\"), sum(\"delta_time\").alias(\"total_delta_time\"))\\\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"route_id\", \"direction\", \"day_type\").orderBy(asc(\"total_distance\"))))\\\n",
    "    .where(\"row_number = 1\")\\\n",
    "    .drop(\"row_number\")\\\n",
    "    .withColumn(\"velocity\", col(\"total_distance\") / col(\"total_delta_time\"))\\\n",
    "    \n",
    "bus_velocities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b50f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to aggregated_data\n",
    "bus_velocities\\\n",
    "    .select(\"route_id\", \"direction\", \"day_type\", \"velocity\", \"velocity_sign\")\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .json(AGGREGATED_DATA_DIR + \"bus_velocities.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15361802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+-------------+------------------+----------------+------------------+\n",
      "|route_id|direction|day_type|velocity_sign|    total_distance|total_delta_time|          velocity|\n",
      "+--------+---------+--------+-------------+------------------+----------------+------------------+\n",
      "|       2|        0|       0|           -1|10315.835773343833|            2031| 5.079190434930494|\n",
      "|       2|        0|       1|           -1|1712.0868527420473|             268| 6.388383778888236|\n",
      "|       2|        1|       0|            1|16075.666055615284|            2526|  6.36407999034651|\n",
      "|       2|        1|       1|            1|3346.4842351421703|             482| 6.942913350917366|\n",
      "|       4|        0|       0|            1| 9399.616383917773|            3005|3.1279921410708065|\n",
      "|       4|        0|       1|            1| 5601.300423440861|             732| 7.652049758798991|\n",
      "|       4|        1|       0|           -1|15953.232621357203|            3160| 5.048491335872533|\n",
      "|       4|        1|       1|           -1|26701.564321827464|            4447| 6.004399442731609|\n",
      "|      10|        0|       0|            1|6903.6093306843295|            1912|3.6106743361319715|\n",
      "|      10|        0|       1|            1| 693.5846655281766|             186| 3.728949814667616|\n",
      "|      10|        1|       0|           -1|4230.5420203423855|            1371|3.0857345152023234|\n",
      "|      10|        1|       1|           -1| 809.7674872959825|             144| 5.623385328444323|\n",
      "|      14|        0|       0|           -1|2151.8116377744345|             655| 3.285208607289213|\n",
      "|      14|        0|       1|           -1|1527.6895465274729|             159| 9.608110355518697|\n",
      "|      14|        1|       0|            1|16252.900865673713|            2891| 5.621895837313633|\n",
      "|      14|        1|       1|            1| 784.2092314628527|             480|1.6337692322142765|\n",
      "|      16|        0|       0|            1|24345.638027411493|            5033| 4.837202071808363|\n",
      "|      16|        0|       1|            1| 658.9193873082077|             156| 4.223842226334664|\n",
      "|      16|        1|       0|           -1|173502.45118974292|           34086|5.0901382148020575|\n",
      "|      16|        1|       1|           -1| 40802.78338154406|            7296| 5.592486757338824|\n",
      "+--------+---------+--------+-------------+------------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bus_velocities\\\n",
    "    .orderBy(\"route_id\", \"direction\", \"day_type\", \"velocity_sign\")\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
