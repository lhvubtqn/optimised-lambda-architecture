{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c964e372",
   "metadata": {},
   "source": [
    "# Static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bf0be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY_TYPE_WEEKDAY = 0\n",
    "DAY_TYPE_WEEKEND = 1\n",
    "DATA_ACTUAL_TIMEZONE = \"America/Los_Angeles\"\n",
    "\n",
    "MAX_SECONDS_SINCE_REPORT = 300\n",
    "MIN_VELOCITY = 1 # m/s\n",
    "MAX_VELOCITY = 18 # m/s\n",
    "\n",
    "STATIC_DATA_DIR = \"hdfs://namenode:8020/ola/static_data/\"\n",
    "HISTORICAL_DATA_DIR = \"hdfs://namenode:8020/ola/historical_data/\"\n",
    "AGGREGATED_DATA_DIR = \"hdfs://namenode:8020/ola/aggregated_data/\"\n",
    "\n",
    "LOCAL_STATIC_DATA_DIR = \"/home/data/static_data/\"\n",
    "LOCAL_HISTORICAL_DATA_DIR = \"/home/data/historical_data_24h/\"\n",
    "LOCAL_AGGREGATED_DATA_DIR = \"/home/data/aggregated_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2b7690",
   "metadata": {},
   "source": [
    "# Init PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import  SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Spark session & context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"spark://0.0.0.0:7077\").setAppName(\"batch-job\")\n",
    "\n",
    "conf.set(\"spark.cores.max\", \"1\")\n",
    "conf.set(\"spark.default.parallelism\", \"2\")\n",
    "\n",
    "conf.set(\"spark.driver.extraClassPath\", \"/usr/local/spark/third-party-jars/*\")\n",
    "conf.set(\"spark.executor.extraClassPath\", \"/usr/local/spark/third-party-jars/*\")\n",
    "conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "conf.set(\"spark.ui.port\", \"4041\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f5fa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       ".container { \n",
       "    width:95% !important; \n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    ".container { \n",
    "    width:95% !important; \n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31b825",
   "metadata": {},
   "source": [
    "# Load static & aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd6794",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06de0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7d4ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if segment is not an empty segment (two vertex is the same)\n",
    "def fis_not_the_same(from_latlon, to_latlon):\n",
    "    return (from_latlon[0] != to_latlon[0]) | (from_latlon[1] != to_latlon[1])\n",
    "\n",
    "def ffile_path_to_ts(file_path):\n",
    "    return int(file_path[-18:-4])\n",
    "\n",
    "def fget_day_type(epoch_seconds):\n",
    "    dt = datetime.fromtimestamp(epoch_seconds)\n",
    "    wd = dt.astimezone(timezone(DATA_ACTUAL_TIMEZONE)).weekday()\n",
    "    return DAY_TYPE_WEEKDAY if wd < 5 else DAY_TYPE_WEEKEND\n",
    "\n",
    "def fhaversine_meter(lat1, lon1, lat2, lon2):\n",
    "    # distance between latitudes and longitudes\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    "\n",
    "    # convert to radians\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    " \n",
    "    # apply formulae\n",
    "    a = (math.pow(math.sin(dLat / 2), 2) +\n",
    "         math.pow(math.sin(dLon / 2), 2) *\n",
    "             math.cos(lat1) * math.cos(lat2));\n",
    "    rad = 6371\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c * 1000\n",
    "\n",
    "def ffind_distance(lat1, lon1, flat1, flon1, slat1, slon1, cl1, lat2, lon2, flat2, flon2, slat2, slon2, cl2):\n",
    "    if cl1 == cl2:\n",
    "        return fhaversine_meter(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "    if cl1 > cl2:\n",
    "        return ffind_distance(lat2, lon2, flat2, flon2, slat2, slon2, cl2, lat1, lon1, flat1, flon1, slat1, slon1, cl1)\n",
    "\n",
    "    return fhaversine_meter(lat1, lon1, slat1, slon1) + cl2 - cl1 - fhaversine_meter(flat2, flon2, slat2, slon2) + fhaversine_meter(flat2, flon2, lat2, lon2)\n",
    "\n",
    "def ffind_velocity_sign(cl1, cl2, lat1, lon1, lat2, lon2, flat, flon, slat, slon):\n",
    "    if cl1 == cl2:\n",
    "        d1 = fhaversine_meter(lat1, lon1, flat, flon)\n",
    "        d2 = fhaversine_meter(lat2, lon2, slat, slon)\n",
    "        d = fhaversine_meter(flat, flon, slat, slon)\n",
    "        return 1 if d1 + d2 < d else -1\n",
    "    return 1 if cl2 > cl1 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2aaff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf\n",
    "is_not_the_same = udf(lambda a, b: ffis_not_the_same(a, b), BooleanType())\n",
    "\n",
    "file_path_to_ts = udf(lambda a: ffile_path_to_ts(a), LongType())\n",
    "\n",
    "get_day_type = udf(lambda a: fget_day_type(a), IntegerType())\n",
    "\n",
    "haversine_meter = udf(lambda a, b, c, d: fhaversine_meter(a, b, c, d), DoubleType())\n",
    "\n",
    "find_distance = udf(lambda a, b, c, d, e, f, g, h, p, q, r, s, t, u: ffind_distance(a, b, c, d, e, f, g, h, p, q, r, s, t, u), DoubleType())\n",
    "\n",
    "find_velocity_sign = udf(lambda a, b, c, d, e, f, g, h, p, q: ffind_velocity_sign(a, b, c, d, e, f, g, h, p, q), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a522d1",
   "metadata": {},
   "source": [
    "### Load static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5435de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- segment_len_meter: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_sequence: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "routes_segments = spark\\\n",
    "    .read\\\n",
    "    .json(STATIC_DATA_DIR + \"route_segments.json\")\\\n",
    "    .withColumn(\"route_id\", col(\"route_id\").cast(\"int\").alias(\"route_id\"))\n",
    "\n",
    "routes_segments.cache()\n",
    "routes_segments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e1f16",
   "metadata": {},
   "source": [
    "# Load and process historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70cc2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- segment_len_meter: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_sequence: long (nullable = true)\n",
      " |-- bus_segment_distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "bus_positions = spark\\\n",
    "    .read\\\n",
    "    .json(HISTORICAL_DATA_DIR + \"*\")\\\n",
    "    .dropna(\"any\")\n",
    "\n",
    "# Cast fields to int\n",
    "bus_positions = bus_positions\\\n",
    "    .select(\n",
    "        (col(\"timestamp\") - col(\"seconds_since_report\")).alias(\"timestamp\"),\n",
    "        col(\"id\").cast(\"int\").alias(\"bus_id\"),\n",
    "        col(\"route_id\").cast(\"int\").alias(\"route_id\"),\n",
    "        \"run_id\", \"latitude\", \"longitude\"\n",
    "    )\\\n",
    "\n",
    "# With positions having same bus_id, run_id, latitude, longitude, order them by timestamp then drop consecutive duplicate rows\n",
    "bus_positions = bus_positions\\\n",
    "   .select(\n",
    "        \"bus_id\", \"latitude\", \"longitude\", \"route_id\", \"run_id\", \"timestamp\",\n",
    "        *[lag(c).over(Window.partitionBy(\"bus_id\", \"route_id\", \"run_id\").orderBy(\"timestamp\")).alias(\"prev_\" + c) for c in [\"latitude\", \"longitude\", \"timestamp\"]]\n",
    "    )\\\n",
    "    .where((col(\"prev_latitude\").isNull()) | (col(\"latitude\") != col(\"prev_latitude\")) | (col(\"longitude\") != col(\"prev_longitude\")))\\\n",
    "    .select(\"bus_id\", \"latitude\", \"longitude\", \"route_id\", \"run_id\", \"timestamp\")\n",
    "\n",
    "# add day_type field, 0 if is weekday, 1 if is weekend or holiday\n",
    "bus_positions = bus_positions\\\n",
    "    .withColumn(\"day_type\", get_day_type(\"timestamp\"))\n",
    "\n",
    "# get segment of bus positions\n",
    "bus_positions = bus_positions\\\n",
    "    .join(routes_segments, \"route_id\")\\\n",
    "    .withColumn(\"bus_segment_distance\", haversine_meter(\"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\") + haversine_meter(\"latitude\", \"longitude\", \"segment_second_lat\", \"segment_second_lon\") - col(\"segment_len_meter\"))\\\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"bus_id\", \"route_id\", \"latitude\", \"longitude\", \"run_id\", \"timestamp\").orderBy(asc(\"bus_segment_distance\"))))\\\n",
    "    .where(\"row_number = 1\")\\\n",
    "    .drop(\"row_number\")\\\n",
    "\n",
    "# drop rows with at least one null column\n",
    "bus_positions = bus_positions.dropna(\"any\")\n",
    "\n",
    "bus_positions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b339afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- next_timestamp: long (nullable = true)\n",
      " |-- next_latitude: double (nullable = true)\n",
      " |-- next_longitude: double (nullable = true)\n",
      " |-- next_segment_first_lat: double (nullable = true)\n",
      " |-- next_segment_first_lon: double (nullable = true)\n",
      " |-- next_segment_second_lat: double (nullable = true)\n",
      " |-- next_segment_second_lon: double (nullable = true)\n",
      " |-- next_segment_cum_len: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with bus positions having same \"bus_id\", \"route_id\", \"run_id\", \"day_type\", sort them in ascending order by \"timestamp\",\n",
    "# then create a segment with every 2 consecutive rows with same \"run_id\".\n",
    "# This ensure that we only calculate velocity for bus positions in the same trip.\n",
    "bus_segments = bus_positions\\\n",
    "    .select(\n",
    "        \"bus_id\", \"route_id\", \"day_type\", \"run_id\", \"timestamp\", \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\", \"segment_id\",\n",
    "        *[lead(column_name).over(Window.partitionBy(\"bus_id\", \"route_id\", \"day_type\", \"run_id\").orderBy(\"timestamp\")).alias(\"next_\" + column_name) for column_name in [\"timestamp\", \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\"]]\n",
    "    )\\\n",
    "    .dropna(\"any\")\\\n",
    "\n",
    "bus_segments.cache()\n",
    "bus_segments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31bf91a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- next_timestamp: long (nullable = true)\n",
      " |-- next_latitude: double (nullable = true)\n",
      " |-- next_longitude: double (nullable = true)\n",
      " |-- next_segment_first_lat: double (nullable = true)\n",
      " |-- next_segment_first_lon: double (nullable = true)\n",
      " |-- next_segment_second_lat: double (nullable = true)\n",
      " |-- next_segment_second_lon: double (nullable = true)\n",
      " |-- next_segment_cum_len: double (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- delta_time: long (nullable = true)\n",
      " |-- velocity_sign: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with each segment, calculate distance and delta time\n",
    "# then find the sign of velocity of this bus on this segment,\n",
    "# 0 means the bus is running alongside with the segment sequence id\n",
    "bus_segments_1 = bus_segments\\\n",
    "    .withColumn(\n",
    "        \"distance\", \n",
    "        find_distance(\n",
    "            \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \n",
    "            \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\", \n",
    "            \"next_latitude\", \"next_longitude\", \"next_segment_first_lat\", \n",
    "            \"next_segment_first_lon\", \"next_segment_second_lat\", \n",
    "            \"next_segment_second_lon\", \"next_segment_cum_len\"\n",
    "        )\n",
    "    )\\\n",
    "    .where(col(\"distance\") != 0)\\\n",
    "    .withColumn(\"delta_time\", col(\"next_timestamp\") - col(\"timestamp\"))\\\n",
    "    .where(col(\"delta_time\") <= MAX_SECONDS_SINCE_REPORT)\\\n",
    "    .where((col(\"distance\") / col(\"delta_time\") >= MIN_VELOCITY) & (col(\"distance\") / col(\"delta_time\") <= MAX_VELOCITY))\\\n",
    "    .withColumn(\"velocity_sign\", find_velocity_sign(\"segment_cum_len\", \"next_segment_cum_len\", \"latitude\", \"longitude\", \"next_latitude\", \"next_longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\"))\\\n",
    "    \n",
    "bus_segments_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69a309bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- velocity_sign: integer (nullable = true)\n",
      " |-- total_distance: double (nullable = true)\n",
      " |-- total_delta_time: long (nullable = true)\n",
      " |-- velocity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bus_velocities = bus_segments_1\\\n",
    "    .withColumn(\"direction\", substring(\"run_id\", -1, 1))\\\n",
    "    .groupBy(\"route_id\", \"direction\", \"day_type\", \"velocity_sign\")\\\n",
    "    .agg(sum(\"distance\").alias(\"total_distance\"), sum(\"delta_time\").alias(\"total_delta_time\"))\\\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"route_id\", \"direction\", \"day_type\").orderBy(asc(\"total_distance\"))))\\\n",
    "    .where(\"row_number = 1\")\\\n",
    "    .drop(\"row_number\")\\\n",
    "    .withColumn(\"velocity\", col(\"total_distance\") / col(\"total_delta_time\"))\\\n",
    "    \n",
    "bus_velocities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6803685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache before write (speed the write function)\n",
    "# use sql because it will run rightaway\n",
    "bus_velocities.createOrReplaceTempView(\"bus_velocities\")\n",
    "spark.sql(\"cache table bus_velocities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29b50f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to aggregated_data\n",
    "bus_velocities\\\n",
    "    .select(\"route_id\", \"direction\", \"day_type\", \"velocity\", \"velocity_sign\")\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .json(AGGREGATED_DATA_DIR + \"bus_velocities.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"uncache table bus_velocities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
