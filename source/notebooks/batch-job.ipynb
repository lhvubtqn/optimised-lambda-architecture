{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_INTERNAL_ADDRESS = \"10.104.0.2\"\n",
    "\n",
    "DAY_TYPE_WEEKDAY = 0\n",
    "DAY_TYPE_WEEKEND = 1\n",
    "DATA_ACTUAL_TIMEZONE = \"America/Los_Angeles\"\n",
    "\n",
    "MAX_SECONDS_SINCE_REPORT = 300\n",
    "MIN_VELOCITY = 1 # m/s\n",
    "MAX_VELOCITY = 18 # m/s\n",
    "\n",
    "STATIC_DATA_DIR = f\"hdfs://{MASTER_INTERNAL_ADDRESS}:8020/ola/static_data/\"\n",
    "HISTORICAL_DATA_DIR = f\"hdfs://{MASTER_INTERNAL_ADDRESS}:8020/ola/historical_data/\"\n",
    "AGGREGATED_DATA_DIR = f\"hdfs://{MASTER_INTERNAL_ADDRESS}:8020/ola/aggregated_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import  SparkContext, SparkConf\n",
    "\n",
    "# Spark session & context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\")\n",
    "conf.setAppName(\"batch-job\")\n",
    "\n",
    "conf.set(\"spark.cores.max\", \"1\")\n",
    "conf.set(\"spark.default.parallelism\", \"2\")\n",
    "conf.set(\"spark.driver.memory\", \"2g\")\n",
    "\n",
    "conf.set(\"spark.driver.extraClassPath\", \"/root/libs/spark-3.1.1-bin-hadoop3.2/third-party-jars/*\")\n",
    "conf.set(\"spark.executor.extraClassPath\", \"/root/libs/spark-3.1.1-bin-hadoop3.2/third-party-jars/*\")\n",
    "\n",
    "conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "conf.set(\"spark.ui.port\", \"4041\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       ".container { \n",
       "    width:95% !important; \n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    ".container { \n",
    "    width:95% !important; \n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load static & aggregated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if segment is not an empty segment (two vertex is the same)\n",
    "def fis_not_the_same(from_latlon, to_latlon):\n",
    "    return (from_latlon[0] != to_latlon[0]) | (from_latlon[1] != to_latlon[1])\n",
    "\n",
    "def ffile_path_to_ts(file_path):\n",
    "    return int(file_path[-18:-4])\n",
    "\n",
    "def fget_day_type(epoch_seconds):\n",
    "    dt = datetime.fromtimestamp(epoch_seconds)\n",
    "    wd = dt.astimezone(timezone(DATA_ACTUAL_TIMEZONE)).weekday()\n",
    "    return DAY_TYPE_WEEKDAY if wd < 5 else DAY_TYPE_WEEKEND\n",
    "\n",
    "def fhaversine_meter(lat1, lon1, lat2, lon2):\n",
    "    # distance between latitudes and longitudes\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    "\n",
    "    # convert to radians\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    " \n",
    "    # apply formulae\n",
    "    a = (math.pow(math.sin(dLat / 2), 2) +\n",
    "         math.pow(math.sin(dLon / 2), 2) *\n",
    "             math.cos(lat1) * math.cos(lat2));\n",
    "    rad = 6371\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c * 1000\n",
    "\n",
    "def ffind_distance(lat1, lon1, flat1, flon1, slat1, slon1, cl1, lat2, lon2, flat2, flon2, slat2, slon2, cl2):\n",
    "    if cl1 == cl2:\n",
    "        return fhaversine_meter(lat1, lon1, lat2, lon2)\n",
    "        \n",
    "    if cl1 > cl2:\n",
    "        return ffind_distance(lat2, lon2, flat2, flon2, slat2, slon2, cl2, lat1, lon1, flat1, flon1, slat1, slon1, cl1)\n",
    "\n",
    "    return fhaversine_meter(lat1, lon1, slat1, slon1) + cl2 - cl1 - fhaversine_meter(flat2, flon2, slat2, slon2) + fhaversine_meter(flat2, flon2, lat2, lon2)\n",
    "\n",
    "def ffind_velocity_sign(cl1, cl2, lat1, lon1, lat2, lon2, flat, flon, slat, slon):\n",
    "    if cl1 == cl2:\n",
    "        d1 = fhaversine_meter(lat1, lon1, flat, flon)\n",
    "        d2 = fhaversine_meter(lat2, lon2, slat, slon)\n",
    "        d = fhaversine_meter(flat, flon, slat, slon)\n",
    "        return 1 if d1 + d2 < d else -1\n",
    "    return 1 if cl2 > cl1 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udf\n",
    "is_not_the_same = udf(lambda a, b: ffis_not_the_same(a, b), BooleanType())\n",
    "\n",
    "file_path_to_ts = udf(lambda a: ffile_path_to_ts(a), LongType())\n",
    "\n",
    "get_day_type = udf(lambda a: fget_day_type(a), IntegerType())\n",
    "\n",
    "haversine_meter = udf(lambda a, b, c, d: fhaversine_meter(a, b, c, d), DoubleType())\n",
    "\n",
    "find_distance = udf(lambda a, b, c, d, e, f, g, h, p, q, r, s, t, u: ffind_distance(a, b, c, d, e, f, g, h, p, q, r, s, t, u), DoubleType())\n",
    "\n",
    "find_velocity_sign = udf(lambda a, b, c, d, e, f, g, h, p, q: ffind_velocity_sign(a, b, c, d, e, f, g, h, p, q), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- segment_len_meter: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_sequence: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "routes_segments = spark\\\n",
    "    .read\\\n",
    "    .json(STATIC_DATA_DIR + \"route_segments.json\")\\\n",
    "    .withColumn(\"route_id\", col(\"route_id\").cast(\"int\").alias(\"route_id\"))\n",
    "\n",
    "routes_segments.cache()\n",
    "routes_segments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- segment_len_meter: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_sequence: long (nullable = true)\n",
      " |-- bus_segment_distance: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "bus_positions = spark\\\n",
    "    .read\\\n",
    "    .json(HISTORICAL_DATA_DIR + \"*\")\\\n",
    "    .dropna(\"any\")\n",
    "\n",
    "# Cast fields to int\n",
    "bus_positions = bus_positions\\\n",
    "    .select(\n",
    "        (col(\"timestamp\") - col(\"seconds_since_report\")).alias(\"timestamp\"),\n",
    "        col(\"id\").cast(\"int\").alias(\"bus_id\"),\n",
    "        col(\"route_id\").cast(\"int\").alias(\"route_id\"),\n",
    "        \"run_id\", \"latitude\", \"longitude\"\n",
    "    )\\\n",
    "\n",
    "# With positions having same bus_id, run_id, latitude, longitude, order them by timestamp then drop consecutive duplicate rows\n",
    "bus_positions = bus_positions\\\n",
    "   .select(\n",
    "        \"bus_id\", \"latitude\", \"longitude\", \"route_id\", \"run_id\", \"timestamp\",\n",
    "        *[lag(c).over(Window.partitionBy(\"bus_id\", \"route_id\", \"run_id\").orderBy(\"timestamp\")).alias(\"prev_\" + c) for c in [\"latitude\", \"longitude\", \"timestamp\"]]\n",
    "    )\\\n",
    "    .where((col(\"prev_latitude\").isNull()) | (col(\"latitude\") != col(\"prev_latitude\")) | (col(\"longitude\") != col(\"prev_longitude\")))\\\n",
    "    .select(\"bus_id\", \"latitude\", \"longitude\", \"route_id\", \"run_id\", \"timestamp\")\n",
    "\n",
    "# add day_type field, 0 if is weekday, 1 if is weekend or holiday\n",
    "bus_positions = bus_positions\\\n",
    "    .withColumn(\"day_type\", get_day_type(\"timestamp\"))\n",
    "\n",
    "# get segment of bus positions\n",
    "bus_positions = bus_positions\\\n",
    "    .join(routes_segments, \"route_id\")\\\n",
    "    .withColumn(\"bus_segment_distance\", haversine_meter(\"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\") + haversine_meter(\"latitude\", \"longitude\", \"segment_second_lat\", \"segment_second_lon\") - col(\"segment_len_meter\"))\\\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"bus_id\", \"route_id\", \"latitude\", \"longitude\", \"run_id\", \"timestamp\").orderBy(asc(\"bus_segment_distance\"))))\\\n",
    "    .where(\"row_number = 1\")\\\n",
    "    .drop(\"row_number\")\\\n",
    "\n",
    "# drop rows with at least one null column\n",
    "bus_positions = bus_positions.dropna(\"any\")\n",
    "\n",
    "bus_positions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- next_timestamp: long (nullable = true)\n",
      " |-- next_latitude: double (nullable = true)\n",
      " |-- next_longitude: double (nullable = true)\n",
      " |-- next_segment_first_lat: double (nullable = true)\n",
      " |-- next_segment_first_lon: double (nullable = true)\n",
      " |-- next_segment_second_lat: double (nullable = true)\n",
      " |-- next_segment_second_lon: double (nullable = true)\n",
      " |-- next_segment_cum_len: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with bus positions having same \"bus_id\", \"route_id\", \"run_id\", \"day_type\", sort them in ascending order by \"timestamp\",\n",
    "# then create a segment with every 2 consecutive rows with same \"run_id\".\n",
    "# This ensure that we only calculate velocity for bus positions in the same trip.\n",
    "bus_segments = bus_positions\\\n",
    "    .select(\n",
    "        \"bus_id\", \"route_id\", \"day_type\", \"run_id\", \"timestamp\", \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\", \"segment_id\",\n",
    "        *[lead(column_name).over(Window.partitionBy(\"bus_id\", \"route_id\", \"day_type\", \"run_id\").orderBy(\"timestamp\")).alias(\"next_\" + column_name) for column_name in [\"timestamp\", \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\"]]\n",
    "    )\\\n",
    "    .dropna(\"any\")\\\n",
    "\n",
    "bus_segments.cache()\n",
    "bus_segments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bus_id: integer (nullable = true)\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- segment_first_lat: double (nullable = true)\n",
      " |-- segment_first_lon: double (nullable = true)\n",
      " |-- segment_second_lat: double (nullable = true)\n",
      " |-- segment_second_lon: double (nullable = true)\n",
      " |-- segment_cum_len: double (nullable = true)\n",
      " |-- segment_id: long (nullable = true)\n",
      " |-- next_timestamp: long (nullable = true)\n",
      " |-- next_latitude: double (nullable = true)\n",
      " |-- next_longitude: double (nullable = true)\n",
      " |-- next_segment_first_lat: double (nullable = true)\n",
      " |-- next_segment_first_lon: double (nullable = true)\n",
      " |-- next_segment_second_lat: double (nullable = true)\n",
      " |-- next_segment_second_lon: double (nullable = true)\n",
      " |-- next_segment_cum_len: double (nullable = true)\n",
      " |-- distance: double (nullable = true)\n",
      " |-- delta_time: long (nullable = true)\n",
      " |-- velocity_sign: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with each segment, calculate distance and delta time\n",
    "# then find the sign of velocity of this bus on this segment,\n",
    "# 0 means the bus is running alongside with the segment sequence id\n",
    "bus_segments_1 = bus_segments\\\n",
    "    .withColumn(\n",
    "        \"distance\", \n",
    "        find_distance(\n",
    "            \"latitude\", \"longitude\", \"segment_first_lat\", \"segment_first_lon\", \n",
    "            \"segment_second_lat\", \"segment_second_lon\", \"segment_cum_len\", \n",
    "            \"next_latitude\", \"next_longitude\", \"next_segment_first_lat\", \n",
    "            \"next_segment_first_lon\", \"next_segment_second_lat\", \n",
    "            \"next_segment_second_lon\", \"next_segment_cum_len\"\n",
    "        )\n",
    "    )\\\n",
    "    .where(col(\"distance\") != 0)\\\n",
    "    .withColumn(\"delta_time\", col(\"next_timestamp\") - col(\"timestamp\"))\\\n",
    "    .where(col(\"delta_time\") <= MAX_SECONDS_SINCE_REPORT)\\\n",
    "    .where((col(\"distance\") / col(\"delta_time\") >= MIN_VELOCITY) & (col(\"distance\") / col(\"delta_time\") <= MAX_VELOCITY))\\\n",
    "    .withColumn(\"velocity_sign\", find_velocity_sign(\"segment_cum_len\", \"next_segment_cum_len\", \"latitude\", \"longitude\", \"next_latitude\", \"next_longitude\", \"segment_first_lat\", \"segment_first_lon\", \"segment_second_lat\", \"segment_second_lon\"))\\\n",
    "    \n",
    "bus_segments_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: integer (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- day_type: integer (nullable = true)\n",
      " |-- velocity_sign: integer (nullable = true)\n",
      " |-- total_distance: double (nullable = true)\n",
      " |-- total_delta_time: long (nullable = true)\n",
      " |-- velocity: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bus_velocities = bus_segments_1\\\n",
    "    .withColumn(\"direction\", substring(\"run_id\", -1, 1))\\\n",
    "    .groupBy(\"route_id\", \"direction\", \"day_type\", \"velocity_sign\")\\\n",
    "    .agg(sum(\"distance\").alias(\"total_distance\"), sum(\"delta_time\").alias(\"total_delta_time\"))\\\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"route_id\", \"direction\", \"day_type\").orderBy(asc(\"total_distance\"))))\\\n",
    "    .where(\"row_number = 1\")\\\n",
    "    .drop(\"row_number\")\\\n",
    "    .withColumn(\"velocity\", col(\"total_distance\") / col(\"total_delta_time\"))\\\n",
    "    \n",
    "bus_velocities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache before write (speed the write function)\n",
    "# use sql because it will run rightaway\n",
    "bus_velocities.createOrReplaceTempView(\"bus_velocities\")\n",
    "spark.sql(\"cache table bus_velocities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to aggregated_data\n",
    "bus_velocities\\\n",
    "    .select(\"route_id\", \"direction\", \"day_type\", \"velocity\", \"velocity_sign\")\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .json(AGGREGATED_DATA_DIR + \"bus_velocities.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"uncache table bus_velocities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+-------------+------------------+----------------+------------------+\n",
      "|route_id|direction|day_type|velocity_sign|    total_distance|total_delta_time|          velocity|\n",
      "+--------+---------+--------+-------------+------------------+----------------+------------------+\n",
      "|       2|        0|       0|           -1|22826.397357083908|            6166|3.7019781636529205|\n",
      "|       2|        1|       0|            1|37646.688184160725|            6167| 6.104538379140704|\n",
      "|       4|        0|       0|            1| 17549.90810148129|            4225| 4.153824402717465|\n",
      "|       4|        1|       0|           -1| 38726.57245688513|            8462| 4.576527116152816|\n",
      "|      10|        0|       0|            1|14333.984739505211|            3479| 4.120145081777871|\n",
      "|      10|        1|       0|           -1|10194.044030633042|            2123| 4.801716453430543|\n",
      "|      14|        0|       0|           -1|  6208.33024270626|            1934| 3.210098367479969|\n",
      "|      14|        1|       0|            1| 52848.48393473895|            7639| 6.918246358782426|\n",
      "|      16|        0|       0|            1|46503.527207605446|            8291| 5.608916561042751|\n",
      "|      16|        1|       0|           -1|168664.30425973938|           32348| 5.214056642133652|\n",
      "|      18|        0|       0|           -1| 94926.61572563063|           19478|4.8735299171183195|\n",
      "|      18|        1|       0|            1| 36384.80289494746|            7394|4.9208551386188075|\n",
      "|      20|        0|       0|            1|21669.335857062255|            6260|3.4615552487319894|\n",
      "|      20|        1|       0|           -1|  87716.8503723951|           17842| 4.916312653984704|\n",
      "|      28|        0|       0|            1|21848.660661747268|            4383| 4.984864399212245|\n",
      "|      28|        1|       0|           -1| 52636.04774289335|           15895| 3.311484601629025|\n",
      "|      30|        0|       0|           -1|119727.10296681874|           22416| 5.341144850411257|\n",
      "|      30|        1|       0|            1|51571.489584857816|            7056| 7.308884578352865|\n",
      "|      33|        0|       0|           -1| 6582.604149716258|            1774|3.7105998589155904|\n",
      "|      33|        1|       0|            1| 9660.162747285545|            2743|3.5217509104212708|\n",
      "+--------+---------+--------+-------------+------------------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bus_velocities\\\n",
    "    .orderBy(\"route_id\", \"direction\", \"day_type\", \"velocity_sign\")\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}